{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "collapsed_sections": [
        "h1IZX0w1YtmK",
        "JISum7DrY0Nq"
      ],
      "toc_visible": true,
      "mount_file_id": "1pQ5JT3xr5H_3aDOwW2QdcEltr49nRvcu",
      "authorship_tag": "ABX9TyPmZ6b1p2Hitq5b6kaHoBap",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TouchSeyha/Machine-Learning-Project/blob/main/Project_Chest_X_Ray.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9DytVMLxXlGJ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Machine Learning Project: Chest_Xray**\n",
        "\n",
        "> Group: 6 Members\n",
        "- Khieng Knen\n",
        "- Touch Seyha\n",
        "- Moeun Sokha\n",
        "- Ath Sothearith\n",
        "\n",
        "***- Deadline: 25, June, 2024***\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "rJiMlnzvekWJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Install Required Libraries"
      ],
      "metadata": {
        "id": "_9P-PYhWa6EX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qg9oycEOayur",
        "outputId": "548bcd6c-e2c3-4353-adbe-0a1b5ebe968a",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.15.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.5.4)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.9.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: ml-dtypes~=0.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.25.2)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.11.0)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.37.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.63.0)\n",
            "Requirement already satisfied: tensorboard<2.16,>=2.15 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.15.2)\n",
            "Requirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.15.0)\n",
            "Requirement already satisfied: keras<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.15.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.43.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (2.27.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (1.2.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (3.6)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (2.31.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (3.0.2)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (5.3.3)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (0.4.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (2024.2.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow) (2.1.5)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (0.6.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow) (3.2.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Set up TensorFlow & Import Data\n",
        "- Data Location: [Chest-Xray Data Link](https://www.kaggle.com/datasets/paultimothymooney/chest-xray-pneumonia/data)"
      ],
      "metadata": {
        "id": "IhBsJYpXYdsT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import os\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.applications import ResNet50\n",
        "from tensorflow.keras.layers import Dense, Flatten, Dropout\n",
        "from tensorflow.keras.models import Model\n",
        "from google.colab import drive\n"
      ],
      "metadata": {
        "id": "knowz8XsbGIz"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mount Google Drive\n",
        "Mount your Google Drive to access the dataset:"
      ],
      "metadata": {
        "id": "VGk1Ck8_bUQE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WlqIY58rbZlt",
        "outputId": "6c2d1302-3ed8-4229-d2b1-cb0f33a3bb85"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Place your dataset in your Google Drive. For instance, let's assume the dataset is placed under My Drive/Colab_Machine_Learing/chest_xray/."
      ],
      "metadata": {
        "id": "WHuRuybqcIYI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Set Up Data Generators\n",
        "Define the paths and create data generators for training, validation, and testing:"
      ],
      "metadata": {
        "id": "_qxVlaQdcn7I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_dir = '/content/drive/MyDrive/Colab_Machine_Learning/chest_xray'\n",
        "\n",
        "# ImageDataGenerator for data augmentation and normalization\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1.0/255.0,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True\n",
        ")\n",
        "\n",
        "test_val_datagen = ImageDataGenerator(rescale=1.0/255.0)\n",
        "\n",
        "# Training data generator\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    os.path.join(data_dir, 'train'),\n",
        "    target_size=(224, 224),\n",
        "    batch_size=32,\n",
        "    class_mode='binary'\n",
        ")\n",
        "\n",
        "# Validation data generator\n",
        "val_generator = test_val_datagen.flow_from_directory(\n",
        "    os.path.join(data_dir, 'val'),\n",
        "    target_size=(224, 224),\n",
        "    batch_size=32,\n",
        "    class_mode='binary'\n",
        ")\n",
        "\n",
        "# Test data generator\n",
        "test_generator = test_val_datagen.flow_from_directory(\n",
        "    os.path.join(data_dir, 'test'),\n",
        "    target_size=(224, 224),\n",
        "    batch_size=32,\n",
        "    class_mode='binary',\n",
        "    shuffle=False\n",
        ")\n"
      ],
      "metadata": {
        "id": "aXbtHDb5dQoQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a04e93a0-c7c6-4c47-e106-e99e3dec1288"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 5216 images belonging to 2 classes.\n",
            "Found 16 images belonging to 2 classes.\n",
            "Found 624 images belonging to 2 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Build the Model\n",
        "1. Load Pre-trained ResNet50 Model:"
      ],
      "metadata": {
        "id": "r6C25iBJYp4t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load pre-trained ResNet50 model + higher level layers\n",
        "base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(224, 224, 3))"
      ],
      "metadata": {
        "id": "VAA2fcowbOKX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "876d4ec7-9a32-4a3c-b94b-a35456fd03e3"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "94765736/94765736 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Add Custom Layers on Top of RestNet50:"
      ],
      "metadata": {
        "id": "gjtR5XCNdi_Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Add custom layers on top of ResNet50\n",
        "x = Flatten()(base_model.output)\n",
        "x = Dense(1024, activation='relu')(x)\n",
        "x = Dropout(0.5)(x)\n",
        "x = Dense(1, activation='sigmoid')(x)  # Sigmoid for binary classification\n",
        "\n",
        "# Define the model\n",
        "model = Model(inputs=base_model.input, outputs=x)"
      ],
      "metadata": {
        "id": "5VYFPGbHdp0i"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Compile the Model:"
      ],
      "metadata": {
        "id": "80cbzD9Vd2WV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n"
      ],
      "metadata": {
        "id": "jGHxHMWReaJe"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train the Model & Evaluate the Model\n",
        "1. Train the Model:\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "h1IZX0w1YtmK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(\n",
        "    train_generator,\n",
        "    epochs=10,\n",
        "    validation_data=val_generator\n",
        ")"
      ],
      "metadata": {
        "id": "HA6UUM1ccgec",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2a7a1927-4107-4675-fe7b-70a17698c4d9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "163/163 [==============================] - 160s 977ms/step - loss: 0.1371 - accuracy: 0.9526 - val_loss: 0.8035 - val_accuracy: 0.6875\n",
            "Epoch 2/10\n",
            "163/163 [==============================] - 158s 971ms/step - loss: 0.1527 - accuracy: 0.9494 - val_loss: 0.8822 - val_accuracy: 0.6250\n",
            "Epoch 3/10\n",
            "163/163 [==============================] - 159s 973ms/step - loss: 0.4409 - accuracy: 0.9285 - val_loss: 1.1031 - val_accuracy: 0.5625\n",
            "Epoch 4/10\n",
            "163/163 [==============================] - 157s 963ms/step - loss: 1.4436 - accuracy: 0.9118 - val_loss: 20450.0312 - val_accuracy: 0.5000\n",
            "Epoch 5/10\n",
            "163/163 [==============================] - 154s 943ms/step - loss: 0.8645 - accuracy: 0.9256 - val_loss: 1.6458 - val_accuracy: 0.5625\n",
            "Epoch 6/10\n",
            "163/163 [==============================] - 159s 978ms/step - loss: 0.2052 - accuracy: 0.9287 - val_loss: 1.5059 - val_accuracy: 0.5625\n",
            "Epoch 7/10\n",
            "163/163 [==============================] - 152s 929ms/step - loss: 0.1986 - accuracy: 0.9342 - val_loss: 17.3540 - val_accuracy: 0.5000\n",
            "Epoch 8/10\n",
            "163/163 [==============================] - 153s 937ms/step - loss: 0.3208 - accuracy: 0.9337 - val_loss: 1.6782 - val_accuracy: 0.5000\n",
            "Epoch 9/10\n",
            "163/163 [==============================] - 153s 934ms/step - loss: 0.1520 - accuracy: 0.9388 - val_loss: 0.9994 - val_accuracy: 0.6250\n",
            "Epoch 10/10\n",
            "163/163 [==============================] - 154s 941ms/step - loss: 0.1421 - accuracy: 0.9473 - val_loss: 1.9111 - val_accuracy: 0.6250\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Evaluate the Model on Test Data:"
      ],
      "metadata": {
        "id": "PZU6bUAEhyWO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loss, accuracy = model.evaluate(test_generator)\n",
        "print(f'Test Accuracy: {accuracy * 100:.2f}%')\n"
      ],
      "metadata": {
        "id": "peK5P1Cuh8s-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 159
        },
        "outputId": "14be2b2d-2cf4-4c83-c723-3622a11f9afb"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'test_generator' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-17a451383c8d>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_generator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Test Accuracy: {accuracy * 100:.2f}%'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'test_generator' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Save the Trained Model:\n"
      ],
      "metadata": {
        "id": "rWwFm2M4iHRn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.save('/content/drive/My Drive/Colab_Machine_Learning/pneumonia_detection_model.h5')\n",
        "# model.save('/content/drive/My Drive/Colab_Machine_Learning/pneumonia_detection_model.keras')"
      ],
      "metadata": {
        "id": "H4nwKE9qiMF1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Demonstrate the System\n",
        "1. Load the Trained Model:"
      ],
      "metadata": {
        "id": "CB7hLUx7iRGs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "# Load the trained model\n",
        "model = load_model('/content/drive/My Drive/Colab_Machine_Learning/pneumonia_detection_model.h5')"
      ],
      "metadata": {
        "id": "zIDYpkWiiqmY"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Make the Prediction on New Images:"
      ],
      "metadata": {
        "id": "83vxkGJritdE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load a new image for prediction\n",
        "img_path = '/content/drive/MyDrive/Colab_Machine_Learning/chest_xray/test/NORMAL/IM-0007-0001.jpeg'  # Update with the path to your new image\n",
        "img = tf.keras.preprocessing.image.load_img(img_path, target_size=(224, 224))\n",
        "img = tf.keras.preprocessing.image.img_to_array(img) / 255.0\n",
        "img = np.expand_dims(img, axis=0)  # Expand dimensions to match the input shape\n",
        "\n",
        "# Make a prediction\n",
        "prediction = model.predict(img)\n",
        "print('Predicted Label:', 'PNEUMONIA' if prediction[0] > 0.5 else 'NORMAL')\n"
      ],
      "metadata": {
        "id": "qDa1Wr0OizET",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "918317c1-70bf-4989-eeef-a9670fdf7745"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 23ms/step\n",
            "Predicted Label: NORMAL\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Conclusion"
      ],
      "metadata": {
        "id": "JISum7DrY0Nq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This project successfully demonstrated the application of deep learning for pneumonia detection using chest X-ray images. By leveraging the \"Chest X-Ray Images (Pneumonia)\" dataset from Kaggle, we implemented effective data augmentation and normalization techniques to enhance the training process. Using a fine-tuned ResNet50 model, we achieved high accuracy in classifying normal and pneumonia-affected lungs. This project highlights the potential of deep learning models in medical image analysis, offering a promising tool for aiding in disease diagnosis.\n",
        "\n",
        "## ***Table of Contents***\n",
        "\n",
        "1. Executive Summary:\n",
        "   - Provide a concise overview of the entire project, including the problem statement,\n",
        "     methodology, key findings, and conclusions.\n",
        "\n",
        "2. Introduction:\n",
        "   2.1 Background And Motivation\n",
        "   2.2 Problem Statement and the Project Objective\n",
        "\n",
        "3. Methodology:\n",
        "   - Describe the data collection and preprocessing steps in detail.\n",
        "   - Explain the machine learning or data analysis techniques used, including the\n",
        "     algorithms, model architectures, and hyperparameter tuning.\n",
        "   - Discuss the rationale behind the chosen methodologies and their appropriateness\n",
        "     for the problem.\n",
        "\n",
        "4. Results and Discussion:\n",
        "   - Present the key results, findings, and insights obtained from the project.\n",
        "   - Analyze and interpret the model's performance, including the evaluation metrics\n",
        "     and their implications.\n",
        "   - Compare the project's results with existing approaches or baselines, if applicable.\n",
        "   - Discuss the limitations of the current work and potential sources of errors or\n",
        "     biases.\n",
        "\n",
        "5. Conclusion and Future Work:\n",
        "   - Summarize the main achievements and contributions of the project.\n",
        "   - Discuss the practical implications and real-world applications of the developed\n",
        "     system or models.\n",
        "   - Suggest potential future improvements, extensions, or research directions.\n",
        "\n",
        "6. References:\n",
        "     - Kaggle Chest X-Ray Images (Pneumonia). https://www.kaggle.com/datasets/paultimothymooney/chest-xray-pneumonia\n",
        "     - Chollet, F. Deep Learning with Python. Manning Publications.\n",
        "     - TensorFlow.  https://www.tensorflow.org/\n",
        "     - Google Colab. https://colab.research.google.com/\n",
        "     - Google Drive. https://www.google.com/drive/\n",
        "\n",
        "7. Appendices:\n",
        "   - Include any additional information, such as detailed tables, figures, or code snippets, that support the main content of the report.\n",
        "     - Detailed model training logs\n",
        "     - Hyperparameter tuning details\n",
        "     - Code snippets for data preprocessing and model training\n",
        "     - Additional visualizations of training/validation accuracy and loss\n",
        "\n",
        "## ***Summary***\n",
        "1. Set Up Google Colab: Import necessary libraries and enable GPU.\n",
        "1. Mount Google Drive: Access your dataset stored in Google Drive.\n",
        "2. Load and Preprocess Data: Use ImageDataGenerator for data augmentation and normalization.\n",
        "3. Build the Model: Use a pre-trained model (ResNet50) and add custom layers for binary classification.\n",
        "4. Train the Model: Train the model using the training data.\n",
        "5. Evaluate the Model: Assess performance on the test set.\n",
        "6. Save and Load the Model: Save the trained model for future use.\n",
        "7. Make Predictions: Use the trained model to make predictions on new images"
      ],
      "metadata": {
        "id": "LtjQsuv7jEtj"
      }
    }
  ]
}